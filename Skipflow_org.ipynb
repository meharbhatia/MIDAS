{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/Saiteja-Reddy/Automatic-Text-Scoring.git\n",
    "import  keras.layers  as  klayers \n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, LSTM, Input, Embedding, GlobalAveragePooling1D, Concatenate, Activation, Lambda, BatchNormalization, Convolution1D, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "from quadratic_weighted_kappa import QWK\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Tensor_layer(Layer):\n",
    "\tdef __init__(self,output_dim,input_dim=None, **kwargs):\n",
    "\t\tself.output_dim=output_dim\n",
    "\t\tself.input_dim=input_dim\n",
    "\t\tif self.input_dim:\n",
    "\t\t\tkwargs['input_shape']=(self.input_dim,)\n",
    "# \t\tprint(\"YAYY\", input_dim, output_dim)\n",
    "\t\tsuper(Neural_Tensor_layer,self).__init__(**kwargs)\n",
    "\n",
    "\tdef call(self,inputs,mask=None):\n",
    "\t\te1=inputs[0]\n",
    "\t\te2=inputs[1]\n",
    "\t\tbatch_size=K.shape(e1)[0]\n",
    "\t\tk=self.output_dim\n",
    "\t\t\n",
    "\n",
    "\t\tfeed_forward=K.dot(K.concatenate([e1,e2]),self.V)\n",
    "\n",
    "\t\tbilinear_tensor_products = [ K.sum((e2 * K.dot(e1, self.W[0])) + self.b, axis=1) ]\n",
    "\n",
    "\t\tfor i in range(k)[1:]:\t\n",
    "\t\t\tbtp=K.sum((e2*K.dot(e1,self.W[i]))+self.b,axis=1)\n",
    "\t\t\tbilinear_tensor_products.append(btp)\n",
    "\n",
    "\t\tresult=K.tanh(K.reshape(K.concatenate(bilinear_tensor_products,axis=0),(batch_size,k))+feed_forward)\n",
    "\n",
    "\t\treturn result\n",
    "    \n",
    "\tdef build(self,input_shape):\n",
    "\t\tmean=0.0\n",
    "\t\tstd=1.0\n",
    "\t\tk=self.output_dim\n",
    "\t\td=self.input_dim\n",
    "\t\t##truncnorm generate continuous random numbers in given range\n",
    "\t\tW_val=stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(k,d,d))\n",
    "\t\tV_val=stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(2*d,k))\n",
    "\t\tself.W=K.variable(W_val)\n",
    "\t\tself.V=K.variable(V_val)\n",
    "\t\tself.b=K.zeros((self.input_dim,))\n",
    "\t\tself.trainable_weights=[self.W,self.V,self.b]    \n",
    "\n",
    "\tdef compute_output_shape(self, input_shape):\n",
    "\t\tbatch_size=input_shape[0][0]\n",
    "\t\treturn(batch_size,self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temporal_Mean_Pooling(Layer): # conversion from (samples,timesteps,features) to (samples,features)\n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tsuper(Temporal_Mean_Pooling,self).__init__(**kwargs)\n",
    "\t\t# masked values in x (number_of_samples,time)\n",
    "\t\tself.supports_masking=True\n",
    "\t\t# Specifies number of dimensions to each layer\n",
    "\t\tself.input_spec=InputSpec(ndim=3)\n",
    "        \n",
    "\tdef call(self,x,mask=None):\n",
    "\t\tif mask is None:\n",
    "\t\t\tmask=K.mean(K.ones_like(x),axis=-1)\n",
    "\n",
    "\t\tmask=K.cast(mask,K.floatx())\n",
    "\t\t\t\t#dimension size single vec/number of samples\n",
    "\t\treturn K.sum(x,axis=-2)/K.sum(mask,axis=-1,keepdims=True)        \n",
    "\n",
    "\tdef compute_mask(self,input,mask):\n",
    "\t\treturn None\n",
    "    \n",
    "    \n",
    "\tdef compute_output_shape(self,input_shape):\n",
    "\t\treturn (input_shape[0],input_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding done\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM=300\n",
    "MAX_NB_WORDS=4000\n",
    "\n",
    "MAX_SEQUENCE_LENGTH=500\n",
    "VALIDATION_SPLIT=0.20\n",
    "DELTA=20\n",
    "\n",
    "texts=[]\n",
    "labels=[]\n",
    "sentences=[]\n",
    "\n",
    "originals = []\n",
    "\n",
    "fp1=open(\"glove.6B.300d.txt\",\"r\", encoding=\"utf-8\")\n",
    "glove_emb={}\n",
    "for line in fp1:\n",
    "\ttemp=line.split(\" \")\n",
    "\tglove_emb[temp[0]]=np.asarray([float(i) for i in temp[1:]])\n",
    "\n",
    "print(\"Embedding done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range min -  0.0  ; range max -  2.0\n"
     ]
    }
   ],
   "source": [
    "essay_type = '10'\n",
    "\n",
    "fp=open(\"data/train_rel_2.tsv\",'r', encoding=\"ascii\", errors=\"ignore\")\n",
    "fp.readline()\n",
    "originals = []\n",
    "for line in fp:\n",
    "    temp=line.split(\"\\t\")\n",
    "    if(temp[1]==essay_type): ## why only 4 ?? - evals in prompt specific fashion\n",
    "        originals.append(float(temp[2]))\n",
    "# print(originals)\n",
    "fp.close()\n",
    "# print(originals)\n",
    "print(\"range min - \", min(originals) , \" ; range max - \", max(originals))\n",
    "\n",
    "range_min = min(originals)\n",
    "range_max = max(originals)\n",
    "\n",
    "# range_min = 1\n",
    "# range_max = 6\n",
    "\n",
    "fp=open(\"data/train_rel_2.tsv\",'r', encoding=\"ascii\", errors=\"ignore\")\n",
    "fp.readline()\n",
    "sentences=[]\n",
    "for line in fp:\n",
    "    temp=line.split(\"\\t\")\n",
    "    if(temp[1]==essay_type): ## why only 4 ?? - evals in prompt specific fashion\n",
    "        texts.append(temp[4])\n",
    "        labels.append((float(temp[2])-range_min)/(range_max-range_min)) ## why ??  - normalize to range [0-1]\n",
    "        line=temp[4].strip()\n",
    "#         print(line)\n",
    "        sentences.append(nltk.tokenize.word_tokenize(line))\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1640\n",
      "1640\n",
      "1640\n"
     ]
    }
   ],
   "source": [
    "labels\n",
    "print(len(labels))\n",
    "print(len(sentences))\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text labels appended 1640\n",
      "[1.  0.5 1.  ... 0.5 0.5 0. ]\n",
      "1640\n"
     ]
    }
   ],
   "source": [
    "print(\"text labels appended %s\" %len(texts))\n",
    "\n",
    "labels=np.asarray(labels)\n",
    "print(labels)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sentences:\n",
    "\ttemp1=np.zeros((1, EMBEDDING_DIM))\n",
    "\tfor w in i:\n",
    "\t\tif(w in glove_emb):\n",
    "\t\t\ttemp1+=glove_emb[w]\n",
    "\ttemp1/=len(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2560 unique tokens.\n",
      "Shape of data tensor: (1640, 500)\n"
     ]
    }
   ],
   "source": [
    "tokenizer=Tokenizer(num_words = MAX_NB_WORDS) #num_words=MAX_NB_WORDS) #limits vocabulory size\n",
    "tokenizer.fit_on_texts(texts) #encoding the text\n",
    "sequences=tokenizer.texts_to_sequences(texts) #returns list of sequences\n",
    "word_index=tokenizer.word_index #dictionary mapping, word and specific token for that word...\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) #padding to max_length\n",
    "\n",
    "\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1640, 500)\n",
      "(1640,)\n",
      "328\n"
     ]
    }
   ],
   "source": [
    "indices=np.arange(data.shape[0]) #with one argument, start=0, step =1\n",
    "print(data.shape)\n",
    "np.random.shuffle(indices)\n",
    "data=data[indices]\n",
    "# print(data.shape)\n",
    "labels=labels[indices]\n",
    "# np.reshape(labels, ())\n",
    "print(labels.shape)\n",
    "validation_size=int(VALIDATION_SPLIT*data.shape[0])\n",
    "print(validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1312, 500)\n",
      "(1312,)\n",
      "(328, 500)\n"
     ]
    }
   ],
   "source": [
    "x_train=data[:-validation_size] #data-validation data\n",
    "print(x_train.shape)\n",
    "# print(x_train)\n",
    "# print(labels)\n",
    "y_train=labels[:-validation_size]\n",
    "# print(y_train.transpose)\n",
    "print(y_train.shape)\n",
    "# y_train = np.reshape(y_train, (1427, 1))\n",
    "# print(y_train_new)\n",
    "# print(y_train)\n",
    "x_val=data[-validation_size:]\n",
    "print(x_val.shape)\n",
    "y_val=labels[-validation_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2560, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index), EMBEDDING_DIM))\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560\n"
     ]
    }
   ],
   "source": [
    "for word,i in word_index.items():\n",
    "\tif(i>=len(word_index)):\n",
    "\t\tcontinue\n",
    "\tif word in glove_emb:\n",
    "\t\t\tembedding_matrix[i]=glove_emb[word]\n",
    "vocab_size=len(word_index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer=Embedding(vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
    "\t\t\t\t\t\t\tmask_zero=True,\n",
    "\t\t\t\t\t\t\ttrainable=False)\n",
    "# print(embedding_layer.shape)\n",
    "side_embedding_layer=Embedding(vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
    "\t\t\t\t\t\t\tmask_zero=False,\n",
    "\t\t\t\t\t\t\ttrainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SKIPFLOW(lstm_dim=50, lr=1e-4, lr_decay=1e-6, k=4, eta=3, delta=50, activation=\"relu\", maxlen=MAX_SEQUENCE_LENGTH, seed=None):\n",
    "    e = Input(name='essay',shape=(maxlen,))\n",
    "    print(\"e\", e)\n",
    "#     trad_feats=Input(shape=(7,))\n",
    "#     print(\"trad_feats\", trad_feats)\n",
    "    embed = embedding_layer(e)\n",
    "    print(embed.shape)\n",
    "    lstm_layer=LSTM(lstm_dim,return_sequences=True)\n",
    "    print(lstm_layer)\n",
    "    hidden_states=lstm_layer(embed)\n",
    "    htm=Temporal_Mean_Pooling()(hidden_states)    \n",
    "    side_embed = side_embedding_layer(e)\n",
    "    side_hidden_states=lstm_layer(side_embed)    \n",
    "    tensor_layer=Neural_Tensor_layer(output_dim=k,input_dim=lstm_dim)\n",
    "#     print(input_dim, output_dim)\n",
    "    pairs = [((eta + i * delta) % maxlen, (eta + i * delta + delta) % maxlen) for i in range(maxlen // delta)]\n",
    "    hidden_pairs = [ (Lambda(lambda t: t[:, p[0], :])(side_hidden_states), Lambda(lambda t: t[:, p[1], :])(side_hidden_states)) for p in pairs]\n",
    "    sigmoid = Dense(1, activation=\"sigmoid\", kernel_initializer=initializers.glorot_normal(seed=seed))\n",
    "    coherence = [sigmoid(tensor_layer([hp[0], hp[1]])) for hp in hidden_pairs]\n",
    "    co_tm=Concatenate()(coherence[:]+[htm])\n",
    "    dense = Dense(256, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(co_tm)\n",
    "    dense = Dense(128, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(dense)\n",
    "    dense = Dense(64, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(dense)\n",
    "    out = Dense(1, activation=\"sigmoid\")(dense)\n",
    "    model = Model(inputs=[e], outputs=[out])\n",
    "    print(\"input\", [e])\n",
    "    print(\"outputs\", out)\n",
    "    adam = Adam(lr=lr, decay=lr_decay)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=[\"MSE\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e Tensor(\"essay_1:0\", shape=(?, 500), dtype=float32)\n",
      "(?, 500, 300)\n",
      "<keras.layers.recurrent.LSTM object at 0x7f05c24db198>\n",
      "input [<tf.Tensor 'essay_1:0' shape=(?, 500) dtype=float32>]\n",
      "outputs Tensor(\"dense_10/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "essay (InputLayer)              (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 500, 300)     768000      essay[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 500, 500)     1602000     embedding_3[0][0]                \n",
      "                                                                 embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_38 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_39 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_40 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 500, 300)     768000      essay[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "neural__tensor_layer_2 (Neural_ (None, 4)            1004500     lambda_21[0][0]                  \n",
      "                                                                 lambda_22[0][0]                  \n",
      "                                                                 lambda_23[0][0]                  \n",
      "                                                                 lambda_24[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_31[0][0]                  \n",
      "                                                                 lambda_32[0][0]                  \n",
      "                                                                 lambda_33[0][0]                  \n",
      "                                                                 lambda_34[0][0]                  \n",
      "                                                                 lambda_35[0][0]                  \n",
      "                                                                 lambda_36[0][0]                  \n",
      "                                                                 lambda_37[0][0]                  \n",
      "                                                                 lambda_38[0][0]                  \n",
      "                                                                 lambda_39[0][0]                  \n",
      "                                                                 lambda_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            5           neural__tensor_layer_2[0][0]     \n",
      "                                                                 neural__tensor_layer_2[1][0]     \n",
      "                                                                 neural__tensor_layer_2[2][0]     \n",
      "                                                                 neural__tensor_layer_2[3][0]     \n",
      "                                                                 neural__tensor_layer_2[4][0]     \n",
      "                                                                 neural__tensor_layer_2[5][0]     \n",
      "                                                                 neural__tensor_layer_2[6][0]     \n",
      "                                                                 neural__tensor_layer_2[7][0]     \n",
      "                                                                 neural__tensor_layer_2[8][0]     \n",
      "                                                                 neural__tensor_layer_2[9][0]     \n",
      "__________________________________________________________________________________________________\n",
      "temporal__mean__pooling_2 (Temp (None, 500)          0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 510)          0           dense_6[0][0]                    \n",
      "                                                                 dense_6[1][0]                    \n",
      "                                                                 dense_6[2][0]                    \n",
      "                                                                 dense_6[3][0]                    \n",
      "                                                                 dense_6[4][0]                    \n",
      "                                                                 dense_6[5][0]                    \n",
      "                                                                 dense_6[6][0]                    \n",
      "                                                                 dense_6[7][0]                    \n",
      "                                                                 dense_6[8][0]                    \n",
      "                                                                 dense_6[9][0]                    \n",
      "                                                                 temporal__mean__pooling_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          130816      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 128)          32896       dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 64)           8256        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1)            65          dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,314,538\n",
      "Trainable params: 2,778,538\n",
      "Non-trainable params: 1,536,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras.utils.vis_utils import plot_model\n",
    "earlystopping = EarlyStopping(monitor=\"val_mean_squared_error\", patience=5)\n",
    "sf_1 = SKIPFLOW(lstm_dim=500, lr=2e-4, lr_decay=2e-6, k=4, eta=13, delta=50, activation=\"relu\", seed=None)\n",
    "sf_1.summary()\n",
    "# plot_model(sf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Train on 1312 samples, validate on 328 samples\n",
      "Epoch 1/100\n",
      "1312/1312 [==============================] - 82s 63ms/step - loss: 0.0912 - mean_squared_error: 0.0912 - val_loss: 0.0774 - val_mean_squared_error: 0.0774\n",
      "Epoch 2/100\n",
      "1312/1312 [==============================] - 76s 58ms/step - loss: 0.0670 - mean_squared_error: 0.0670 - val_loss: 0.0709 - val_mean_squared_error: 0.0709\n",
      "Epoch 3/100\n",
      "1312/1312 [==============================] - 76s 58ms/step - loss: 0.0618 - mean_squared_error: 0.0618 - val_loss: 0.0715 - val_mean_squared_error: 0.0715\n",
      "Epoch 4/100\n",
      "1312/1312 [==============================] - 77s 59ms/step - loss: 0.0583 - mean_squared_error: 0.0583 - val_loss: 0.0686 - val_mean_squared_error: 0.0686\n",
      "Epoch 5/100\n",
      "1312/1312 [==============================] - 75s 57ms/step - loss: 0.0542 - mean_squared_error: 0.0542 - val_loss: 0.0635 - val_mean_squared_error: 0.0635\n",
      "Epoch 6/100\n",
      "1312/1312 [==============================] - 76s 58ms/step - loss: 0.0533 - mean_squared_error: 0.0533 - val_loss: 0.0654 - val_mean_squared_error: 0.0654\n",
      "Epoch 7/100\n",
      "1312/1312 [==============================] - 76s 58ms/step - loss: 0.0478 - mean_squared_error: 0.0478 - val_loss: 0.0705 - val_mean_squared_error: 0.0705\n",
      "Epoch 8/100\n",
      "1312/1312 [==============================] - 77s 59ms/step - loss: 0.0506 - mean_squared_error: 0.0506 - val_loss: 0.0655 - val_mean_squared_error: 0.0655\n",
      "Epoch 9/100\n",
      "1312/1312 [==============================] - 75s 57ms/step - loss: 0.0488 - mean_squared_error: 0.0488 - val_loss: 0.0605 - val_mean_squared_error: 0.0605\n",
      "Epoch 10/100\n",
      "1312/1312 [==============================] - 77s 58ms/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0678 - val_mean_squared_error: 0.0678\n",
      "Epoch 11/100\n",
      "1312/1312 [==============================] - 76s 58ms/step - loss: 0.0436 - mean_squared_error: 0.0436 - val_loss: 0.0604 - val_mean_squared_error: 0.0604\n",
      "Epoch 12/100\n",
      "1312/1312 [==============================] - 76s 58ms/step - loss: 0.0382 - mean_squared_error: 0.0382 - val_loss: 0.0623 - val_mean_squared_error: 0.0623\n",
      "Epoch 13/100\n",
      "1312/1312 [==============================] - 76s 58ms/step - loss: 0.0382 - mean_squared_error: 0.0382 - val_loss: 0.0654 - val_mean_squared_error: 0.0654\n",
      "Epoch 14/100\n",
      "1312/1312 [==============================] - 77s 59ms/step - loss: 0.0395 - mean_squared_error: 0.0395 - val_loss: 0.0678 - val_mean_squared_error: 0.0678\n",
      "Epoch 15/100\n",
      "1312/1312 [==============================] - 75s 57ms/step - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0659 - val_mean_squared_error: 0.0659\n",
      "Epoch 16/100\n",
      "1312/1312 [==============================] - 76s 58ms/step - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0611 - val_mean_squared_error: 0.0611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f05c3372f60>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(sf_1)\n",
    "epochs = 100\n",
    "# epochs = 1000\n",
    "print(type(x_train))\n",
    "# y_train = np.asarray(y_train)\n",
    "print(type(y_train))\n",
    "\n",
    "sf_1.fit(x_train, y_train, batch_size=32, epochs=epochs, validation_data=([x_val], y_val), callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.51576495],\n",
       "       [0.9658915 ],\n",
       "       [0.47671583],\n",
       "       [0.91740537],\n",
       "       [0.42550012],\n",
       "       [0.43072212],\n",
       "       [0.9679698 ],\n",
       "       [0.93470377],\n",
       "       [0.08579108],\n",
       "       [0.46050602],\n",
       "       [0.50039715],\n",
       "       [0.62147593],\n",
       "       [0.19804919],\n",
       "       [0.36389017],\n",
       "       [0.8850249 ],\n",
       "       [0.5059503 ],\n",
       "       [0.05699658],\n",
       "       [0.95597506],\n",
       "       [0.9810964 ],\n",
       "       [0.4625445 ],\n",
       "       [0.96305126],\n",
       "       [0.963956  ],\n",
       "       [0.9697412 ],\n",
       "       [0.49433717],\n",
       "       [0.47930503],\n",
       "       [0.4753928 ],\n",
       "       [0.50283283],\n",
       "       [0.02202386],\n",
       "       [0.57478154],\n",
       "       [0.9709096 ],\n",
       "       [0.50503075],\n",
       "       [0.52067983],\n",
       "       [0.9449668 ],\n",
       "       [0.08881131],\n",
       "       [0.4514292 ],\n",
       "       [0.44479707],\n",
       "       [0.48355597],\n",
       "       [0.48702008],\n",
       "       [0.47781163],\n",
       "       [0.9753257 ],\n",
       "       [0.4284686 ],\n",
       "       [0.97958887],\n",
       "       [0.42631036],\n",
       "       [0.3769688 ],\n",
       "       [0.9657771 ],\n",
       "       [0.89152646],\n",
       "       [0.1464966 ],\n",
       "       [0.42372912],\n",
       "       [0.9687867 ],\n",
       "       [0.76170266],\n",
       "       [0.9818706 ],\n",
       "       [0.9403236 ],\n",
       "       [0.84198976],\n",
       "       [0.50882685],\n",
       "       [0.45115885],\n",
       "       [0.9146192 ],\n",
       "       [0.48489678],\n",
       "       [0.48908928],\n",
       "       [0.98775285],\n",
       "       [0.446614  ],\n",
       "       [0.49246207],\n",
       "       [0.8758794 ],\n",
       "       [0.9838774 ],\n",
       "       [0.41659483],\n",
       "       [0.5240504 ],\n",
       "       [0.17706773],\n",
       "       [0.83602786],\n",
       "       [0.44985276],\n",
       "       [0.599903  ],\n",
       "       [0.09961751],\n",
       "       [0.59534085],\n",
       "       [0.99117124],\n",
       "       [0.44431683],\n",
       "       [0.9780438 ],\n",
       "       [0.5010586 ],\n",
       "       [0.87340057],\n",
       "       [0.6145894 ],\n",
       "       [0.98864734],\n",
       "       [0.5699508 ],\n",
       "       [0.48194167],\n",
       "       [0.9570373 ],\n",
       "       [0.46797848],\n",
       "       [0.55800295],\n",
       "       [0.9787218 ],\n",
       "       [0.5948378 ],\n",
       "       [0.3760187 ],\n",
       "       [0.9877656 ],\n",
       "       [0.19472691],\n",
       "       [0.51687217],\n",
       "       [0.2982697 ],\n",
       "       [0.988509  ],\n",
       "       [0.48841998],\n",
       "       [0.99400663],\n",
       "       [0.4289167 ],\n",
       "       [0.7864561 ],\n",
       "       [0.1835781 ],\n",
       "       [0.86530524],\n",
       "       [0.92177355],\n",
       "       [0.7104384 ],\n",
       "       [0.9138099 ],\n",
       "       [0.94663894],\n",
       "       [0.46760347],\n",
       "       [0.1342875 ],\n",
       "       [0.47477156],\n",
       "       [0.9869967 ],\n",
       "       [0.5148576 ],\n",
       "       [0.49108285],\n",
       "       [0.96059704],\n",
       "       [0.49859038],\n",
       "       [0.54471797],\n",
       "       [0.41007957],\n",
       "       [0.61546755],\n",
       "       [0.12872139],\n",
       "       [0.752507  ],\n",
       "       [0.36545324],\n",
       "       [0.95681286],\n",
       "       [0.9753283 ],\n",
       "       [0.44151887],\n",
       "       [0.36246234],\n",
       "       [0.9853832 ],\n",
       "       [0.9622532 ],\n",
       "       [0.01729003],\n",
       "       [0.19837198],\n",
       "       [0.2103554 ],\n",
       "       [0.45954823],\n",
       "       [0.09342572],\n",
       "       [0.58092666],\n",
       "       [0.54328626],\n",
       "       [0.45984426],\n",
       "       [0.47899947],\n",
       "       [0.6673141 ],\n",
       "       [0.8919548 ],\n",
       "       [0.9842191 ],\n",
       "       [0.5414864 ],\n",
       "       [0.46088356],\n",
       "       [0.4638144 ],\n",
       "       [0.61793923],\n",
       "       [0.48931122],\n",
       "       [0.48739052],\n",
       "       [0.5974254 ],\n",
       "       [0.47297645],\n",
       "       [0.85693884],\n",
       "       [0.2925948 ],\n",
       "       [0.9931476 ],\n",
       "       [0.48228264],\n",
       "       [0.49258077],\n",
       "       [0.11658335],\n",
       "       [0.6008426 ],\n",
       "       [0.16407529],\n",
       "       [0.98629975],\n",
       "       [0.93239725],\n",
       "       [0.05093214],\n",
       "       [0.50939727],\n",
       "       [0.4034593 ],\n",
       "       [0.9954836 ],\n",
       "       [0.8123411 ],\n",
       "       [0.30020857],\n",
       "       [0.96645737],\n",
       "       [0.9705342 ],\n",
       "       [0.9794992 ],\n",
       "       [0.14950117],\n",
       "       [0.48148432],\n",
       "       [0.4229588 ],\n",
       "       [0.5544793 ],\n",
       "       [0.49245322],\n",
       "       [0.3462093 ],\n",
       "       [0.38189474],\n",
       "       [0.10004851],\n",
       "       [0.99263835],\n",
       "       [0.427534  ],\n",
       "       [0.36610854],\n",
       "       [0.44376707],\n",
       "       [0.46302477],\n",
       "       [0.22161996],\n",
       "       [0.03048688],\n",
       "       [0.5840627 ],\n",
       "       [0.24234903],\n",
       "       [0.98708975],\n",
       "       [0.9802762 ],\n",
       "       [0.42927662],\n",
       "       [0.478749  ],\n",
       "       [0.9757992 ],\n",
       "       [0.9940164 ],\n",
       "       [0.48762155],\n",
       "       [0.47862062],\n",
       "       [0.96483064],\n",
       "       [0.69015706],\n",
       "       [0.19256908],\n",
       "       [0.44071096],\n",
       "       [0.9613589 ],\n",
       "       [0.9851751 ],\n",
       "       [0.99518037],\n",
       "       [0.04683408],\n",
       "       [0.49411237],\n",
       "       [0.8434251 ],\n",
       "       [0.98725796],\n",
       "       [0.4577281 ],\n",
       "       [0.9822664 ],\n",
       "       [0.5186592 ],\n",
       "       [0.5242759 ],\n",
       "       [0.979384  ],\n",
       "       [0.9810033 ],\n",
       "       [0.01633009],\n",
       "       [0.74850005],\n",
       "       [0.9899262 ],\n",
       "       [0.52855396],\n",
       "       [0.9501747 ],\n",
       "       [0.53236437],\n",
       "       [0.9847306 ],\n",
       "       [0.95805824],\n",
       "       [0.9961553 ],\n",
       "       [0.899618  ],\n",
       "       [0.6768895 ],\n",
       "       [0.96180236],\n",
       "       [0.26909587],\n",
       "       [0.70011646],\n",
       "       [0.5028411 ],\n",
       "       [0.428709  ],\n",
       "       [0.45866835],\n",
       "       [0.43658468],\n",
       "       [0.32305825],\n",
       "       [0.5042273 ],\n",
       "       [0.04177326],\n",
       "       [0.45467144],\n",
       "       [0.9809761 ],\n",
       "       [0.9833382 ],\n",
       "       [0.98960197],\n",
       "       [0.4095027 ],\n",
       "       [0.5414162 ],\n",
       "       [0.98224753],\n",
       "       [0.38458937],\n",
       "       [0.991898  ],\n",
       "       [0.9535353 ],\n",
       "       [0.9570994 ],\n",
       "       [0.94909257],\n",
       "       [0.5010583 ],\n",
       "       [0.48783094],\n",
       "       [0.9960729 ],\n",
       "       [0.46931595],\n",
       "       [0.44151986],\n",
       "       [0.4976734 ],\n",
       "       [0.34777296],\n",
       "       [0.98480797],\n",
       "       [0.9279807 ],\n",
       "       [0.48922604],\n",
       "       [0.9857697 ],\n",
       "       [0.06883422],\n",
       "       [0.4876228 ],\n",
       "       [0.49515074],\n",
       "       [0.42726022],\n",
       "       [0.91745216],\n",
       "       [0.3332949 ],\n",
       "       [0.9918294 ],\n",
       "       [0.3752314 ],\n",
       "       [0.26565436],\n",
       "       [0.43039382],\n",
       "       [0.5058643 ],\n",
       "       [0.46161425],\n",
       "       [0.87068486],\n",
       "       [0.50250816],\n",
       "       [0.46345112],\n",
       "       [0.10788947],\n",
       "       [0.44350904],\n",
       "       [0.45148262],\n",
       "       [0.41206703],\n",
       "       [0.5127929 ],\n",
       "       [0.43549213],\n",
       "       [0.45535013],\n",
       "       [0.7330075 ],\n",
       "       [0.5851143 ],\n",
       "       [0.19378743],\n",
       "       [0.9844278 ],\n",
       "       [0.7932683 ],\n",
       "       [0.8131728 ],\n",
       "       [0.4536453 ],\n",
       "       [0.4814107 ],\n",
       "       [0.16588423],\n",
       "       [0.5320502 ],\n",
       "       [0.12383017],\n",
       "       [0.96046555],\n",
       "       [0.15417048],\n",
       "       [0.9816617 ],\n",
       "       [0.76811194],\n",
       "       [0.45722866],\n",
       "       [0.51786643],\n",
       "       [0.01898471],\n",
       "       [0.49825513],\n",
       "       [0.98835146],\n",
       "       [0.94946307],\n",
       "       [0.9362289 ],\n",
       "       [0.44832656],\n",
       "       [0.96861523],\n",
       "       [0.9815591 ],\n",
       "       [0.4944858 ],\n",
       "       [0.47823787],\n",
       "       [0.2137413 ],\n",
       "       [0.43659297],\n",
       "       [0.18451953],\n",
       "       [0.71049184],\n",
       "       [0.36744487],\n",
       "       [0.9794654 ],\n",
       "       [0.74799734],\n",
       "       [0.9706913 ],\n",
       "       [0.47498003],\n",
       "       [0.9873343 ],\n",
       "       [0.48299778],\n",
       "       [0.42208037],\n",
       "       [0.07864839],\n",
       "       [0.95807105],\n",
       "       [0.46738413],\n",
       "       [0.9464294 ],\n",
       "       [0.7154057 ],\n",
       "       [0.15759012],\n",
       "       [0.9903166 ],\n",
       "       [0.1136989 ],\n",
       "       [0.04694742],\n",
       "       [0.9834285 ],\n",
       "       [0.45119458],\n",
       "       [0.51369303],\n",
       "       [0.5084727 ],\n",
       "       [0.5109063 ],\n",
       "       [0.13786364],\n",
       "       [0.9774853 ],\n",
       "       [0.43475178],\n",
       "       [0.98401916],\n",
       "       [0.916628  ],\n",
       "       [0.07612741],\n",
       "       [0.43132624]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=sf_1.predict([x_val])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 0, 2, 1, 1, 2, 1, 0, 1, 0, 0, 1, 0, 2, 1, 0, 2, 0, 1, 1, 2, 2, 1, 1, 1, 1, 0, 1, 0, 2, 1, 2, 0, 1, 1, 2, 1, 1, 1, 0, 2, 1, 1, 1, 2, 0, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 0, 2, 1, 1, 1, 1, 1, 0, 2, 1, 2, 2, 2, 1, 1, 2, 1, 0, 2, 1, 0, 2, 1, 1, 1, 2, 1, 2, 0, 2, 0, 2, 1, 2, 0, 2, 1, 0, 1, 2, 1, 1, 2, 1, 0, 1, 1, 1, 2, 0, 1, 2, 1, 1, 2, 0, 0, 1, 0, 1, 0, 1, 2, 1, 2, 1, 1, 2, 1, 0, 1, 1, 1, 0, 2, 1, 2, 1, 2, 1, 1, 0, 1, 0, 2, 1, 0, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 0, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 0, 2, 0, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1, 0, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 0, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 1, 1, 2, 0, 2, 1, 2, 2, 2, 1, 2, 1, 1, 0, 1, 2, 2, 1, 2, 0, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 0, 0, 2, 0, 2, 0, 1, 1, 0, 1, 2, 2, 1, 0, 2, 2, 1, 1, 1, 1, 1, 1, 0, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 2, 0, 1, 0, 0, 2, 1, 1, 1, 1, 0, 2, 1, 2, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "y_val_fin = [int(round(a*(range_max-range_min)+range_min)) for a in y_val]\n",
    "print(y_val_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 1, 2, 1, 1, 2, 2, 0, 1, 1, 1, 0, 1, 2, 1, 0, 2, 2, 1, 2, 2, 2, 1, 1, 1, 1, 0, 1, 2, 1, 1, 2, 0, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 0, 1, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 0, 2, 1, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 0, 1, 1, 2, 1, 2, 1, 2, 0, 2, 2, 1, 2, 2, 1, 0, 1, 2, 1, 1, 2, 1, 1, 1, 1, 0, 2, 1, 2, 2, 1, 1, 2, 2, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 0, 1, 0, 2, 2, 0, 1, 1, 2, 2, 1, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0, 0, 1, 0, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 0, 1, 2, 2, 2, 0, 1, 2, 2, 1, 2, 1, 1, 2, 2, 0, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 0, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 2, 1, 1, 0, 1, 0, 2, 0, 2, 2, 1, 1, 0, 1, 2, 2, 2, 1, 2, 2, 1, 1, 0, 1, 0, 1, 1, 2, 1, 2, 1, 2, 1, 1, 0, 2, 1, 2, 1, 0, 2, 0, 0, 2, 1, 1, 1, 1, 0, 2, 1, 2, 2, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred_fin =[int(round(a*(range_max-range_min)+range_min)) for a in y_pred.reshape(328).tolist()]\n",
    "print(y_pred_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6933831897252479\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(y_val_fin,y_pred_fin,weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cmatrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def QWK_new(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = Cmatrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6933831897252478"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QWK_new(y_val_fin, y_pred_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf_1.save('model_final/10_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0315299 ],\n",
       "       [1.931783  ],\n",
       "       [0.95343167],\n",
       "       [1.8348107 ],\n",
       "       [0.85100025],\n",
       "       [0.86144423],\n",
       "       [1.9359396 ],\n",
       "       [1.8694075 ],\n",
       "       [0.17158216],\n",
       "       [0.92101204],\n",
       "       [1.0007943 ],\n",
       "       [1.2429519 ],\n",
       "       [0.39609838],\n",
       "       [0.72778034],\n",
       "       [1.7700498 ],\n",
       "       [1.0119005 ],\n",
       "       [0.11399317],\n",
       "       [1.9119501 ],\n",
       "       [1.9621928 ],\n",
       "       [0.925089  ],\n",
       "       [1.9261025 ],\n",
       "       [1.927912  ],\n",
       "       [1.9394825 ],\n",
       "       [0.98867434],\n",
       "       [0.95861006],\n",
       "       [0.9507856 ],\n",
       "       [1.0056657 ],\n",
       "       [0.04404771],\n",
       "       [1.1495631 ],\n",
       "       [1.9418192 ],\n",
       "       [1.0100615 ],\n",
       "       [1.0413597 ],\n",
       "       [1.8899336 ],\n",
       "       [0.17762262],\n",
       "       [0.9028584 ],\n",
       "       [0.88959414],\n",
       "       [0.96711195],\n",
       "       [0.97404015],\n",
       "       [0.95562327],\n",
       "       [1.9506514 ],\n",
       "       [0.8569372 ],\n",
       "       [1.9591777 ],\n",
       "       [0.8526207 ],\n",
       "       [0.7539376 ],\n",
       "       [1.9315542 ],\n",
       "       [1.7830529 ],\n",
       "       [0.2929932 ],\n",
       "       [0.84745824],\n",
       "       [1.9375734 ],\n",
       "       [1.5234053 ],\n",
       "       [1.9637412 ],\n",
       "       [1.8806472 ],\n",
       "       [1.6839795 ],\n",
       "       [1.0176537 ],\n",
       "       [0.9023177 ],\n",
       "       [1.8292384 ],\n",
       "       [0.96979356],\n",
       "       [0.97817856],\n",
       "       [1.9755057 ],\n",
       "       [0.893228  ],\n",
       "       [0.98492414],\n",
       "       [1.7517588 ],\n",
       "       [1.9677548 ],\n",
       "       [0.83318967],\n",
       "       [1.0481008 ],\n",
       "       [0.35413545],\n",
       "       [1.6720557 ],\n",
       "       [0.8997055 ],\n",
       "       [1.199806  ],\n",
       "       [0.19923502],\n",
       "       [1.1906817 ],\n",
       "       [1.9823425 ],\n",
       "       [0.88863367],\n",
       "       [1.9560876 ],\n",
       "       [1.0021172 ],\n",
       "       [1.7468011 ],\n",
       "       [1.2291788 ],\n",
       "       [1.9772947 ],\n",
       "       [1.1399016 ],\n",
       "       [0.96388334],\n",
       "       [1.9140747 ],\n",
       "       [0.93595695],\n",
       "       [1.1160059 ],\n",
       "       [1.9574436 ],\n",
       "       [1.1896756 ],\n",
       "       [0.7520374 ],\n",
       "       [1.9755312 ],\n",
       "       [0.38945383],\n",
       "       [1.0337443 ],\n",
       "       [0.5965394 ],\n",
       "       [1.977018  ],\n",
       "       [0.97683996],\n",
       "       [1.9880133 ],\n",
       "       [0.8578334 ],\n",
       "       [1.5729122 ],\n",
       "       [0.3671562 ],\n",
       "       [1.7306105 ],\n",
       "       [1.8435471 ],\n",
       "       [1.4208767 ],\n",
       "       [1.8276198 ],\n",
       "       [1.8932779 ],\n",
       "       [0.93520695],\n",
       "       [0.268575  ],\n",
       "       [0.9495431 ],\n",
       "       [1.9739934 ],\n",
       "       [1.0297152 ],\n",
       "       [0.9821657 ],\n",
       "       [1.9211941 ],\n",
       "       [0.99718076],\n",
       "       [1.0894359 ],\n",
       "       [0.82015914],\n",
       "       [1.2309351 ],\n",
       "       [0.25744277],\n",
       "       [1.505014  ],\n",
       "       [0.7309065 ],\n",
       "       [1.9136257 ],\n",
       "       [1.9506567 ],\n",
       "       [0.88303775],\n",
       "       [0.7249247 ],\n",
       "       [1.9707664 ],\n",
       "       [1.9245064 ],\n",
       "       [0.03458005],\n",
       "       [0.39674395],\n",
       "       [0.4207108 ],\n",
       "       [0.91909647],\n",
       "       [0.18685144],\n",
       "       [1.1618533 ],\n",
       "       [1.0865725 ],\n",
       "       [0.9196885 ],\n",
       "       [0.95799893],\n",
       "       [1.3346282 ],\n",
       "       [1.7839096 ],\n",
       "       [1.9684381 ],\n",
       "       [1.0829728 ],\n",
       "       [0.9217671 ],\n",
       "       [0.9276288 ],\n",
       "       [1.2358785 ],\n",
       "       [0.97862244],\n",
       "       [0.97478104],\n",
       "       [1.1948508 ],\n",
       "       [0.9459529 ],\n",
       "       [1.7138777 ],\n",
       "       [0.5851896 ],\n",
       "       [1.9862952 ],\n",
       "       [0.9645653 ],\n",
       "       [0.98516154],\n",
       "       [0.2331667 ],\n",
       "       [1.2016852 ],\n",
       "       [0.32815057],\n",
       "       [1.9725995 ],\n",
       "       [1.8647945 ],\n",
       "       [0.10186428],\n",
       "       [1.0187945 ],\n",
       "       [0.8069186 ],\n",
       "       [1.9909672 ],\n",
       "       [1.6246822 ],\n",
       "       [0.60041714],\n",
       "       [1.9329147 ],\n",
       "       [1.9410684 ],\n",
       "       [1.9589984 ],\n",
       "       [0.29900235],\n",
       "       [0.96296865],\n",
       "       [0.8459176 ],\n",
       "       [1.1089586 ],\n",
       "       [0.98490644],\n",
       "       [0.6924186 ],\n",
       "       [0.7637895 ],\n",
       "       [0.20009702],\n",
       "       [1.9852767 ],\n",
       "       [0.855068  ],\n",
       "       [0.7322171 ],\n",
       "       [0.88753414],\n",
       "       [0.92604953],\n",
       "       [0.44323993],\n",
       "       [0.06097376],\n",
       "       [1.1681254 ],\n",
       "       [0.48469806],\n",
       "       [1.9741795 ],\n",
       "       [1.9605525 ],\n",
       "       [0.85855323],\n",
       "       [0.957498  ],\n",
       "       [1.9515984 ],\n",
       "       [1.9880328 ],\n",
       "       [0.9752431 ],\n",
       "       [0.95724124],\n",
       "       [1.9296613 ],\n",
       "       [1.3803141 ],\n",
       "       [0.38513815],\n",
       "       [0.8814219 ],\n",
       "       [1.9227178 ],\n",
       "       [1.9703501 ],\n",
       "       [1.9903607 ],\n",
       "       [0.09366816],\n",
       "       [0.98822474],\n",
       "       [1.6868502 ],\n",
       "       [1.9745159 ],\n",
       "       [0.9154562 ],\n",
       "       [1.9645329 ],\n",
       "       [1.0373183 ],\n",
       "       [1.0485518 ],\n",
       "       [1.958768  ],\n",
       "       [1.9620066 ],\n",
       "       [0.03266019],\n",
       "       [1.4970001 ],\n",
       "       [1.9798524 ],\n",
       "       [1.0571079 ],\n",
       "       [1.9003494 ],\n",
       "       [1.0647287 ],\n",
       "       [1.9694612 ],\n",
       "       [1.9161165 ],\n",
       "       [1.9923106 ],\n",
       "       [1.799236  ],\n",
       "       [1.353779  ],\n",
       "       [1.9236047 ],\n",
       "       [0.53819174],\n",
       "       [1.4002329 ],\n",
       "       [1.0056822 ],\n",
       "       [0.857418  ],\n",
       "       [0.9173367 ],\n",
       "       [0.87316936],\n",
       "       [0.6461165 ],\n",
       "       [1.0084546 ],\n",
       "       [0.08354652],\n",
       "       [0.9093429 ],\n",
       "       [1.9619522 ],\n",
       "       [1.9666764 ],\n",
       "       [1.9792039 ],\n",
       "       [0.8190054 ],\n",
       "       [1.0828325 ],\n",
       "       [1.9644951 ],\n",
       "       [0.76917875],\n",
       "       [1.983796  ],\n",
       "       [1.9070706 ],\n",
       "       [1.9141988 ],\n",
       "       [1.8981851 ],\n",
       "       [1.0021166 ],\n",
       "       [0.9756619 ],\n",
       "       [1.9921458 ],\n",
       "       [0.9386319 ],\n",
       "       [0.8830397 ],\n",
       "       [0.9953468 ],\n",
       "       [0.6955459 ],\n",
       "       [1.9696159 ],\n",
       "       [1.8559614 ],\n",
       "       [0.9784521 ],\n",
       "       [1.9715394 ],\n",
       "       [0.13766843],\n",
       "       [0.9752456 ],\n",
       "       [0.9903015 ],\n",
       "       [0.85452044],\n",
       "       [1.8349043 ],\n",
       "       [0.6665898 ],\n",
       "       [1.9836588 ],\n",
       "       [0.7504628 ],\n",
       "       [0.5313087 ],\n",
       "       [0.86078763],\n",
       "       [1.0117286 ],\n",
       "       [0.9232285 ],\n",
       "       [1.7413697 ],\n",
       "       [1.0050163 ],\n",
       "       [0.92690223],\n",
       "       [0.21577895],\n",
       "       [0.8870181 ],\n",
       "       [0.90296525],\n",
       "       [0.82413405],\n",
       "       [1.0255858 ],\n",
       "       [0.87098426],\n",
       "       [0.91070026],\n",
       "       [1.466015  ],\n",
       "       [1.1702286 ],\n",
       "       [0.38757485],\n",
       "       [1.9688556 ],\n",
       "       [1.5865366 ],\n",
       "       [1.6263456 ],\n",
       "       [0.9072906 ],\n",
       "       [0.9628214 ],\n",
       "       [0.33176845],\n",
       "       [1.0641004 ],\n",
       "       [0.24766034],\n",
       "       [1.9209311 ],\n",
       "       [0.30834097],\n",
       "       [1.9633234 ],\n",
       "       [1.5362239 ],\n",
       "       [0.9144573 ],\n",
       "       [1.0357329 ],\n",
       "       [0.03796941],\n",
       "       [0.99651027],\n",
       "       [1.9767029 ],\n",
       "       [1.8989261 ],\n",
       "       [1.8724577 ],\n",
       "       [0.8966531 ],\n",
       "       [1.9372305 ],\n",
       "       [1.9631182 ],\n",
       "       [0.9889716 ],\n",
       "       [0.95647573],\n",
       "       [0.4274826 ],\n",
       "       [0.87318593],\n",
       "       [0.36903906],\n",
       "       [1.4209837 ],\n",
       "       [0.73488975],\n",
       "       [1.9589309 ],\n",
       "       [1.4959947 ],\n",
       "       [1.9413826 ],\n",
       "       [0.94996005],\n",
       "       [1.9746686 ],\n",
       "       [0.96599555],\n",
       "       [0.84416074],\n",
       "       [0.15729678],\n",
       "       [1.9161421 ],\n",
       "       [0.93476826],\n",
       "       [1.8928587 ],\n",
       "       [1.4308114 ],\n",
       "       [0.31518024],\n",
       "       [1.9806333 ],\n",
       "       [0.2273978 ],\n",
       "       [0.09389484],\n",
       "       [1.966857  ],\n",
       "       [0.90238917],\n",
       "       [1.0273861 ],\n",
       "       [1.0169454 ],\n",
       "       [1.0218126 ],\n",
       "       [0.27572727],\n",
       "       [1.9549706 ],\n",
       "       [0.86950356],\n",
       "       [1.9680383 ],\n",
       "       [1.833256  ],\n",
       "       [0.15225482],\n",
       "       [0.8626525 ]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred*(range_max-range_min)+range_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_1.save_weights('weights_final/10_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in /home/rajivratn/anaconda3/lib/python3.7/site-packages (1.4.1)\n",
      "Collecting pyparsing>=2.1.4\n",
      "  Using cached pyparsing-2.4.6-py2.py3-none-any.whl (67 kB)\n",
      "Installing collected packages: pyparsing\n",
      "Successfully installed pyparsing-2.4.6\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-ebe9e4a76306>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# !pip install graphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msf_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         raise ImportError(\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;34m'Failed to import `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;34m'Please install `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "!pip install pydot\n",
    "# !pip install graphviz\n",
    "from keras.utils import plot_model\n",
    "plot_model(sf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding done\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7132881562982345"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.7132881562982345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7132881562982345 1\n",
    "0.5567800670819358 2\n",
    "0.5590659472768016 3\n",
    "0.6475944737402457 4\n",
    "0.705901006844428  5\n",
    "0.7511181679935593 6\n",
    "0.5626685892575236 7\n",
    "0.5539630085049697 8\n",
    "\n",
    "0.6933831897252478 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
